{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install pdpipe \n!pip install symspellpy\n!pip install pycontractions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nimport string\n# !pip install symspellpy\nimport pkg_resources\nfrom symspellpy.symspellpy import SymSpell\nfrom symspellpy import SymSpell, Verbosity\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport csv\nimport pandas as pd\n\nimport nltk\nnltk.download('punkt')\n\n#Contraction Import\nfrom pycontractions import Contractions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests, json, csv\nimport re\n\ndef remove_wrong_abb(key,value):\n    if(re.search('it means',value)):\n        return False;\n    elif(re.search('\\*',value)):\n        return False\n    elif(value==\"\"):\n        return False\n    elif(len(key)>7):\n        return False\n    else :\n        return True\n    \nresp = requests.get(\"http://www.netlingo.com/acronyms.php\")\nsoup = BeautifulSoup(resp.text, \"html.parser\")\nslangdict= {}\nkey=\"\"\nvalue=\"\"\nfor div in soup.findAll('div', attrs={'class':'list_box3'}):\n    for li in div.findAll('li'):\n        for a in li.findAll('a'):\n            key =a.text\n            value = li.text.split(key)[1]\n            if(remove_wrong_abb(key,value)):\n                if(re.search('\\ -or- ',value)):\n                    pos=re.search('\\ -or- ',value).start()\n                    slangdict[key]=value[:pos]\n                else:\n                    slangdict[key]=value\n\n                    \n# with open('myslang.json', 'w') as f:\n#     json.dump(slangdict, f, indent=2)\n\nw = csv.writer(open(\"myslang.csv\", \"w\"))\nfor key, val in slangdict.items():\n    w.writerow([key.lower(), val.lower()])\n    \nreader = csv.reader(open('myslang.csv', 'r'))\nabbreviations = {}\nfor row in reader:\n    k, v = row\n    abbreviations[k] = v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sym_spell_4space = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\ndictionary_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nsym_spell_4space.load_dictionary(dictionary_path, term_index=0, count_index=1)\n\n\nsym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\nbigram_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\nsym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\nsym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont = Contractions(api_key=\"glove-twitter-100\")\ncont.load_models()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Version Edit: Sonam D.\ndef to_lower(text):\n    text = text.lower()\n    return text\n\n#Version Edit: Anit G.\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+|pic.twitter.com\\S+')\n    return url.sub(r' ',text)\n\n#Version Edit: Anit G.\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r' ',text)\n\n#Version Edit: Anit G.\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' ', text)\n\n#Version Edit: Sonam D.\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#Version Edit: Sonam D.\ndef remove_numbers(text):\n    text = re.sub(r'\\d+', ' ', text)\n    return text\n\n#Version Edit: Sonam D.\ndef remove_mentions(text):\n    text = re.sub(r'@\\w*', ' ', text)\n    return text\n\n#Version Edit: Sonam D.\ndef remove_punctuations(text):\n    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_square_bracket(text):\n    text = re.sub('\\[.*?\\]', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_angular_bracket(text):\n    text = re.sub('\\<.*?\\>+', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_newline(text):\n    text = re.sub('\\n', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_words_with_numbers(text):\n    text = re.sub('\\w*\\d\\w*', ' ', text)\n    return text\n    \n#Version Edit: Susanth D.\ndef hashtag_to_words(text):\n    text = re.sub(r'##', '#', text)\n    hash_pattern = re.compile(r\"#\\w*\")\n    hashtag_list = re.findall(r\"#\\w+\",text)\n    for hashtag in hashtag_list:\n        hashtag = re.sub(r'#', '', hashtag)\n        text = re.sub(hashtag, sym_spell_4space.word_segmentation(hashtag).corrected_string, text)\n    text = re.sub(r'#', '', text)\n#     text = re.sub(r'# ', '', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_stopwords(text):\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        if token not in stopwords.words('english'):\n            textop = textop + token + ' '\n    return textop\n\n#Version Edit: Sonam D.\ndef correct_misspelled_with_context(text):\n    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n    text = str(suggestions[0])\n    text = re.sub(r', \\d', ' ', text)\n#     text = remove_numbers(text)\n    return text\n\n#Version Edit: Sonam D.\ndef stemming_text(text):\n    stemmer= PorterStemmer()\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        textop = textop + stemmer.stem(token) + ' '\n    return textop\n\n#Version Edit: Sonam D.\ndef lemmatization(text):\n    lemmatizer=WordNetLemmatizer()\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        textop = textop + lemmatizer.lemmatize(token) + ' '\n    return textop\n\n#Version Edit: Anit G.\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n#Version Edit: Anit G.\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\n#Version Edit: Sonam D.\n# def handle_contractions(text):\n#     text = re.sub(r\"’\", \"'\", text)\n#     for word in text.split():\n#         if word.lower() in contractions:\n#             text = text.replace(word, contractions[word.lower()])\n#     return(text)\n\n#Version Edit: Saurabh M.\ndef removeRepeated(tweet):\n    prev = ''\n    tweet_new = ''\n    for c in tweet:\n        caps = False\n        if c.isdigit():\n            tweet_new += c\n            continue\n        if c.isalpha() == True:\n            if ord(c) >= 65 and ord(c)<=90:\n                caps = True\n            c = c.lower()\n            if c == prev:\n                count += 1\n            else:\n                count = 1\n                prev = c\n            if count >= 3:\n                continue\n            if caps == True:\n                tweet_new += c.upper()\n            else:\n                tweet_new += c\n        else:\n            tweet_new += c\n    return tweet_new\n\n\ndef Expand_Contractions(text):\n    return list(cont.expand_texts([text]))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = []\n# pipeline.append(to_lower)\npipeline.append(remove_newline)\npipeline.append(remove_URL)\npipeline.append(remove_html)\npipeline.append(remove_emoji)\n# pipeline.append(hashtag_to_words)\n# pipeline.append(remove_words_with_numbers)\n# pipeline.append(remove_numbers)\npipeline.append(remove_mentions)\npipeline.append(remove_square_bracket)\npipeline.append(remove_angular_bracket)\npipeline.append(Expand_Contractions)\n# pipeline.append(remove_punctuations)\npipeline.append(removeRepeated)\n# pipeline.append(convert_abbrev_in_text)\n# pipeline.append(remove_stopwords)\n# pipeline.append(correct_misspelled_with_context)\n# pipeline.append(remove_numbers)\n# pipeline.append(remove_stopwords)\n# # pipeline.append(stemming_text)\npipeline.append(lemmatization)\n\ntrain = preprocess_pipeline(pipeline, 'text', train)\ntest = preprocess_pipeline(pipeline, 'text', test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_COLUMN = 'text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train[TEXT_COLUMN].astype(str)\ny_train = train[TARGET_COLUMN].values\nx_test = test[TEXT_COLUMN].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 300\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_MODELS = 2\nBATCH_SIZE = 512\nEPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_predictions = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,y_train,\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=2\n        )\n        checkpoint_predictions.append(model.predict(x_test, batch_size=BATCH_SIZE))\n        weights.append(2 ** global_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.average(checkpoint_predictions, weights=weights, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.iloc[:, 1] = (predictions > 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(sub['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}