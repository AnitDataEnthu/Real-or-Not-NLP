{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries that I need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\r\n",
      "  Downloading bert-for-tf2-0.13.5.tar.gz (40 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.8 MB/s \r\n",
      "\u001b[?25hCollecting py-params>=0.7.3\r\n",
      "  Downloading py-params-0.8.3.tar.gz (4.5 kB)\r\n",
      "Collecting params-flow>=0.7.1\r\n",
      "  Downloading params-flow-0.7.4.tar.gz (19 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (1.18.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (4.42.0)\r\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\r\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.13.5-py3-none-any.whl size=29946 sha256=854ccbf000fc6a917c7298b565bc2ef82c0871ed9835daf49a5be28d504d30b9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/00/4a/e3ae98003e155850dee523617ced7794e6e3da07d27e3802a0\r\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.8.3-py3-none-any.whl size=4690 sha256=a7a01a7ebe863c7b4c9e16f51cde1db7e105613e601c9e1b3b72788b85799953\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/14/29/7adda9e71ff1d1aff49809918f7d37d8325683ced69860c527\r\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.7.4-py3-none-any.whl size=16194 sha256=8acb8b351542cd08689f254b4273bac356116e126ec2dc4e5d3f2fec0d0027b3\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/90/ca/a5d500c088762773489e532acd0268226cf2597f211928f38a\r\n",
      "Successfully built bert-for-tf2 py-params params-flow\r\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\r\n",
      "Successfully installed bert-for-tf2-0.13.5 params-flow-0.7.4 py-params-0.8.3\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.85)\r\n",
      "Collecting pdpipe\r\n",
      "  Downloading pdpipe-0.0.46-py3-none-any.whl (47 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.0 MB/s \r\n",
      "\u001b[?25hCollecting strct\r\n",
      "  Downloading strct-0.0.30-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting skutil>=0.0.15\r\n",
      "  Downloading skutil-0.0.16-py2.py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.6/site-packages (from pdpipe) (2.1.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pdpipe) (4.42.0)\r\n",
      "Requirement already satisfied: pandas>=0.18.0 in /opt/conda/lib/python3.6/site-packages (from pdpipe) (0.25.3)\r\n",
      "Collecting decore\r\n",
      "  Downloading decore-0.0.1.tar.gz (19 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from skutil>=0.0.15->pdpipe) (1.18.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.18.0->pdpipe) (2.8.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.18.0->pdpipe) (2019.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.18.0->pdpipe) (1.14.0)\r\n",
      "Building wheels for collected packages: decore\r\n",
      "  Building wheel for decore (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for decore: filename=decore-0.0.1-py2.py3-none-any.whl size=4190 sha256=0a4d9adb2602cd2420ef1d5db589b0d9743f35da3798e793efbb4a665254822a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/40/2a/59306076795b4b9b35a210a859053a479061021810c546ddbe\r\n",
      "Successfully built decore\r\n",
      "Installing collected packages: strct, decore, skutil, pdpipe\r\n",
      "Successfully installed decore-0.0.1 pdpipe-0.0.46 skutil-0.0.16 strct-0.0.30\r\n",
      "Collecting symspellpy\r\n",
      "  Downloading symspellpy-6.5.2-py3-none-any.whl (2.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 2.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /opt/conda/lib/python3.6/site-packages (from symspellpy) (1.18.1)\r\n",
      "Installing collected packages: symspellpy\r\n",
      "Successfully installed symspellpy-6.5.2\r\n",
      "Collecting pycontractions\r\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (0.5.1)\r\n",
      "Requirement already satisfied: gensim>=2.0 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (3.8.1)\r\n",
      "Collecting language-check>=1.0\r\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pyemd>=0.4.4->pycontractions) (1.18.1)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.9.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.12.13)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.22.0)\r\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.49.0)\r\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.13 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.15.13)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.3.3)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.5)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.11.28)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.24.3)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.13->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.13->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.0)\r\n",
      "Building wheels for collected packages: language-check\r\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=90190895 sha256=4708b779d40490a781aaf537fd27d008f009630f40f117b93bd6b483027395d4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/fe/32/3b83a67c4f1182f7f6aa134c1d04cdcd893072bdadb4f5a64c\r\n",
      "Successfully built language-check\r\n",
      "Installing collected packages: language-check, pycontractions\r\n",
      "Successfully installed language-check-1.1 pycontractions-2.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "!pip install pdpipe \n",
    "!pip install symspellpy\n",
    "!pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "\n",
    "\n",
    "#data pipeline \n",
    "import pdpipe as pdp\n",
    "\n",
    "#Other Preprocessing\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import string\n",
    "# !pip install symspellpy\n",
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Contraction Import\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the text and storing it... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping done for Abbreviations and then preprocessed\n",
    "Version Edit: Anit G.\n",
    "* Removed Abusive words\n",
    "* Removed empty slangs\n",
    "* Removed slangs such as \"?- it means some body is asking question\"\n",
    "* Removed slangs with length more than 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, json, csv\n",
    "import re\n",
    "\n",
    "def remove_wrong_abb(key,value):\n",
    "    if(re.search('it means',value)):\n",
    "        return False;\n",
    "    elif(re.search('\\*',value)):\n",
    "        return False\n",
    "    elif(value==\"\"):\n",
    "        return False\n",
    "    elif(len(key)>7):\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "    \n",
    "resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "slangdict= {}\n",
    "key=\"\"\n",
    "value=\"\"\n",
    "for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "    for li in div.findAll('li'):\n",
    "        for a in li.findAll('a'):\n",
    "            key =a.text\n",
    "            value = li.text.split(key)[1]\n",
    "            if(remove_wrong_abb(key,value)):\n",
    "                if(re.search('\\ -or- ',value)):\n",
    "                    pos=re.search('\\ -or- ',value).start()\n",
    "                    slangdict[key]=value[:pos]\n",
    "                else:\n",
    "                    slangdict[key]=value\n",
    "\n",
    "                    \n",
    "# with open('myslang.json', 'w') as f:\n",
    "#     json.dump(slangdict, f, indent=2)\n",
    "\n",
    "w = csv.writer(open(\"myslang.csv\", \"w\"))\n",
    "for key, val in slangdict.items():\n",
    "    w.writerow([key.lower(), val.lower()])\n",
    "    \n",
    "reader = csv.reader(open('myslang.csv', 'r'))\n",
    "abbreviations = {}\n",
    "for row in reader:\n",
    "    k, v = row\n",
    "    abbreviations[k] = v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Preprocessing Configuration and Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SymSpell - Unbeatable Spell Checker\n",
    "\n",
    "**Configuration Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Version Edit: Sonam D.\n",
    "sym_spell_4space = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell_4space.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "sym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "sym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trained for Contraction handling - pycontractions\n",
    "\n",
    "**Configuration Setup**\n",
    "[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 11.8% 45.8/387.1MB downloaded"
     ]
    }
   ],
   "source": [
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing  Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Version Edit: Sonam D.\n",
    "def to_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r' ',text)\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_mentions(text):\n",
    "    text = re.sub(r'@\\w*', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_square_bracket(text):\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_angular_bracket(text):\n",
    "    text = re.sub('<.*?>+', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_newline(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_words_with_numbers(text):\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    return text\n",
    "    \n",
    "#Version Edit: Sonam D.\n",
    "def hashtag_to_words(text):\n",
    "    text = re.sub(r'##', '#', text)\n",
    "    hash_pattern = re.compile(r\"#\\w*\")\n",
    "    hashtag_list = re.findall(hash_pattern,text)\n",
    "    for hashtag in hashtag_list:\n",
    "        hashtag = re.sub(r'#', '', hashtag)\n",
    "        text = re.sub(hashtag, sym_spell_4space.word_segmentation(hashtag).corrected_string, text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'# ', '', text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Susanth D.\n",
    "def remove_stopwords(text):\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            textop = textop + token + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def correct_misspelled_with_context(text):\n",
    "    suggestions = sym_spell_misspelled.lookup_compound(text, max_edit_distance=2)\n",
    "    text = str(suggestions[0])\n",
    "    text = re.sub(r', \\d', ' ', text)\n",
    "#     text = remove_numbers(text)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def stemming_text(text):\n",
    "    stemmer= PorterStemmer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + stemmer.stem(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "def lemmatization(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text_tokens=word_tokenize(text)\n",
    "    textop = ''\n",
    "    for token in text_tokens:\n",
    "        textop = textop + lemmatizer.lemmatize(token) + ' '\n",
    "    return textop\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev(word):\n",
    "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
    "\n",
    "#Version Edit: Anit G.\n",
    "def convert_abbrev_in_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [convert_abbrev(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "#Version Edit: Sonam D.\n",
    "# def handle_contractions(text):\n",
    "#     text = re.sub(r\"’\", \"'\", text)\n",
    "#     for word in text.split():\n",
    "#         if word.lower() in contractions:\n",
    "#             text = text.replace(word, contractions[word.lower()])\n",
    "#     return(text)\n",
    "\n",
    "#Version Edit: Saurabh M.\n",
    "def removeRepeated(tweet):\n",
    "    prev = ''\n",
    "    tweet_new = ''\n",
    "    for c in tweet:\n",
    "        caps = False\n",
    "        if c.isdigit():\n",
    "            tweet_new += c\n",
    "            continue\n",
    "        if c.isalpha() == True:\n",
    "            if ord(c) >= 65 and ord(c)<=90:\n",
    "                caps = True\n",
    "            c = c.lower()\n",
    "            if c == prev:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 1\n",
    "                prev = c\n",
    "            if count >= 3:\n",
    "                continue\n",
    "            if caps == True:\n",
    "                tweet_new += c.upper()\n",
    "            else:\n",
    "                tweet_new += c\n",
    "        else:\n",
    "            tweet_new += c\n",
    "    return tweet_new\n",
    "\n",
    "\n",
    "def Expand_Contractions(text):\n",
    "    return list(cont.expand_texts([text]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Data Pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest financial independence retire early spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon decor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                       happened terrible car crash \n",
       "1   2     NaN      NaN  heard earthquake different city stay safe ever...\n",
       "2   3     NaN      NaN  forest financial independence retire early spo...\n",
       "3   9     NaN      NaN              apocalypse lighting spokane wildfire \n",
       "4  11     NaN      NaN                   typhoon decor kill china taiwan "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline=pdp.ApplyByCols('text',handle_contractions,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_URL,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_html,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_mentions,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_newline,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_emoji,'text')\n",
    "# #pipeline+=pdp.ApplyByCols('text',hashtag_to_words,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_punctuations,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',removeRepeated,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',convert_abbrev_in_text,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',lemmatization,'text')\n",
    "# pipeline+=pdp.ApplyByCols('text',remove_stopwords,'text')\n",
    "\n",
    "\n",
    "pipeline=pdp.ApplyByCols('text',to_lower,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_newline,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_URL,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_html,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_emoji,'text')\n",
    "#pipeline+=pdp.ApplyByCols('text',hashtag_to_words,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_words_with_numbers,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_numbers,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_mentions,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_square_bracket,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_angular_bracket,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',Expand_Contractions,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_punctuations,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',removeRepeated,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',convert_abbrev_in_text,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',correct_misspelled_with_context,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_numbers,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',remove_stopwords,'text')\n",
    "#pipeline+=pdp.ApplyByCols('text',stemming_text,'text')\n",
    "pipeline+=pdp.ApplyByCols('text',lemmatization,'text')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df=pd.concat([train,test],sort=False)\n",
    "train=pipeline(train)\n",
    "test=pipeline(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model [Implementation 1]-  Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT Methods Predefined\"\"\"\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "def build_model(bert_layer,learning_rate=2e-5,Dropout_num=0, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    if Dropout_num == 0:\n",
    "        # Without Dropout\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    else:\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\n",
    "        x = Dropout(Dropout_num)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usuall Preprocessing Methods Not Compatible with pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "# df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "# df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "# df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "# df[\"text\"] = df[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n",
    "# #Case Correction\n",
    "# #Spell Checker\n",
    "# #Stop Words Remover\n",
    "# #Abbreviation remover\n",
    "# #Missing Spaces\n",
    "# #Case Correction (Althoug we cant just do this. This will change the context)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7613 rows and 5 columns in train\n",
      "There are 3263 rows and 4 columns in train\n"
     ]
    }
   ],
   "source": [
    "print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Implementation\n",
    "\n",
    "### Downloading a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT from the Tensorflow Hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "# module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "#https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\n",
    "#https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/1\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenising Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation 1\n",
    "\n",
    "**Version Edit : Anit G.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_BERT = build_model(bert_layer,learning_rate=9e-6,Dropout_num=0, max_len=160)\n",
    "model_BERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/3\n",
      "6851/6851 [==============================] - 476s 69ms/sample - loss: 0.4717 - accuracy: 0.7831 - val_loss: 0.4022 - val_accuracy: 0.8281\n",
      "Epoch 2/3\n",
      "6851/6851 [==============================] - 403s 59ms/sample - loss: 0.3589 - accuracy: 0.8527 - val_loss: 0.4228 - val_accuracy: 0.8189\n",
      "Epoch 3/3\n",
      "6851/6851 [==============================] - 403s 59ms/sample - loss: 0.2829 - accuracy: 0.8883 - val_loss: 0.5398 - val_accuracy: 0.7979\n"
     ]
    }
   ],
   "source": [
    "# Anit's model\n",
    "\n",
    "checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "train_history = model_BERT.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split = 0.1,\n",
    "    epochs = 3, # recomended 3-5 epochs\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size = 16 # recomended 8-16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation 2\n",
    "**Version Edit : Sushant D. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import SGD\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate=1e-7\n",
    "# Dropout_num=0\n",
    "# max_len=160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "# input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "# segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "# _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "# clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "# ## Type1\n",
    "# # out1 = Dense(784, activation='relu', kernel_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "# # out2 = Dense(100, activation='relu', kernel_regularizer=regularizers.l2(9e-5))(out1)\n",
    "\n",
    "# ## Type2\n",
    "# out2 = Dense(768, activation='relu', activity_regularizer=regularizers.l2(9e-5))(clf_output)\n",
    "# out = Dense(1, activation='sigmoid')(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "# # sBERT.compile(SGD(lr=learning_rate, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# sBERT.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# sBERT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "#                                min_delta=0,\n",
    "#                                patience=3,\n",
    "#                                verbose=1, \n",
    "#                                mode='auto')\n",
    "\n",
    "# checkpoint = ModelCheckpoint('sBERT.h5',\n",
    "#                              monitor='val_loss',\n",
    "#                              save_best_only=True)\n",
    "\n",
    "\n",
    "# train_history = sBERT.fit(\n",
    "#     train_input, train_labels,\n",
    "#     validation_split = 0.3,\n",
    "#     epochs = 10, # recomended 3-5 epochs\n",
    "#     callbacks=[checkpoint,early_stopping],\n",
    "#     batch_size = 32 # recomended 8-16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Susanth's Full model\n",
    "# test_pred = sBERT.predict(test_input)\n",
    "# sub['target'] = test_pred.round().astype(int)\n",
    "# sub.to_csv('submission full.csv', index=False)\n",
    "# sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Susanth's checkpoint model\n",
    "# sBERT.load_weights('sBERT.h5')\n",
    "# test_pred = sBERT.predict(test_input)\n",
    "# sub['target'] = test_pred.round().astype(int)\n",
    "# sub.to_csv('submission checkpoint.csv', index=False)\n",
    "# sub.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters Tried and Respective Accuracies [MI-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Tune Parameter 1 ##################\n",
    "# Dropout_num = 0\n",
    "# learning_rate = 1e-7\n",
    "# valid = 0.3\n",
    "# epochs_num = 10\n",
    "# batch_size_num = 32\n",
    "# Dense Layer = 1(Final)\n",
    "# Val_accuracy : \n",
    "# Test_accuracy : \n",
    "######################################################\n",
    "################## Tune Parameter 2 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 9e-6\n",
    "#valid = 0.2\n",
    "#epochs_num = 5\n",
    "#batch_size_num = 32\n",
    "#Val_accuracy : 83.45\n",
    "######################################################\n",
    "################## Tune Parameter 3 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 9e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 3\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : \n",
    "######################################################\n",
    "################## Tune Parameter 4 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 5e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 3\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : 81.63\n",
    "######################################################\n",
    "################## Tune Parameter 5 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 5e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 5\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : \n",
    "######################################################\n",
    "################## Tune Parameter 6 ##################\n",
    "#Dropout_num = 0\n",
    "#learning_rate = 9e-6\n",
    "#valid = 0.1\n",
    "#epochs_num = 5\n",
    "#batch_size_num = 16\n",
    "#Val_accuracy : \n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction by BERT model with my tuning\n",
    "# model_BERT.load_weights('model_BERT.h5')\n",
    "# test_pred_BERT = model_BERT.predict(test_input)\n",
    "# test_pred_BERT_int = test_pred_BERT.round().astype('int')\n",
    "# # Prediction by BERT model with my tuning for the training data - for the Confusion Matrix\n",
    "# train_pred_BERT = model_BERT.predict(train_input)\n",
    "# train_pred_BERT_int = train_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Submission File with MI-1\n",
    "**Version Edit: Anit G.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_BERT.load_weights('model_BERT.h5')\n",
    "test_pred = model_BERT.predict(test_input)\n",
    "sub['target'] = test_pred.round().astype(int)\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Submission File with MI-2\n",
    "**Version Edit: Susanth D.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
